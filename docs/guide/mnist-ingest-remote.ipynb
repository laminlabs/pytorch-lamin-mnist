{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30a10bd4-6e2b-42ce-aa04-01fdc1d5f7ab",
   "metadata": {},
   "source": [
    "# Ingesting a remote ML dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "db5a2cdb-6d02-4f7e-a95c-58ac16fe2251",
   "metadata": {},
   "source": [
    "As part of our demonstration of a simple ML workflow with LaminDB and PyTorch, we now demonstrate a basic process for ingesting data stored remotely (S3 bucket) into a LaminDB instance.\n",
    "\n",
    "```{note}\n",
    "- For an introduction to this four-part demonstration, please see [LaminDB use case: integrating with PyTorch to train a model on the MNIST dataset](./mnist-intro.ipynb).\n",
    "- For ingesting the same dataset stored locally, please see [Ingesting a remote ML dataset](./mnist-ingest-local.ipynb).\n",
    "- For building the PyTorch `Dataset` and training the autoencoder, please see [Integrating with PyTorch and training an autoencoder](./mnist-train.ipynb).\n",
    "- For extending the LaminDB schema, please see [Extending the LaminDB schema](./mnist-extend-schema.ipynb).\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51953b4c-8e71-4e7a-a1ab-c7984fa6c4e4",
   "metadata": {},
   "source": [
    "## Creating a LaminDB instance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23e4f7d-9301-4440-b793-c8a2f8c2df8d",
   "metadata": {},
   "source": [
    "Our first step is to create a LaminDB instance and ingest the local files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f723505b-8140-498f-96e7-d7da0cc12797",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lndb\n",
    "\n",
    "lndb.init(name=\"mnist-remote\", storage=\"s3://bernardo-test-bucket-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781e91a9-dec6-4856-a9be-a11e4b2015cd",
   "metadata": {},
   "source": [
    "Let's take a look at our set up instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e51b416-5777-497c-be2b-d394aea48141",
   "metadata": {},
   "outputs": [],
   "source": [
    "lndb.settings.instance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022f9d1f-70e8-41de-b45e-57d5a6a9c4a5",
   "metadata": {},
   "source": [
    "Now that the instance has been set up with the existing storage, we must ingest the relevant data objects from storage into the instance so that we are able to track and query them.\n",
    "\n",
    "During this step, LaminDB commits object metadata to the instance database (in this case, a local SQLite instance), which serves as the metadata and governance layer of our [data lakehouse](https://www.databricks.com/glossary/data-lakehouse)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b85984c-395a-4c34-8f45-c9d9894654c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lamindb as ln\n",
    "\n",
    "ln.nb.header()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0fe60e-ec48-483b-9122-d248e29130a2",
   "metadata": {},
   "source": [
    "```{note}\n",
    "\n",
    "`ln.nb` is an access point of one our open-source modules, nbproject.\n",
    "\n",
    "The call to `ln.nb.header()` initializes the notebook and enables key data provenance features.\n",
    "\n",
    "Whenever a data object is ingested into the instance from an initialized notebook, LaminDB automatically identifies the notebook where it came from and inserts the relevant provenance records in the database.\n",
    "\n",
    "For more details, check out our guide on [ingest and tracking data from notebook runs](https://lamin.ai/docs/db/guide/nb).\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1855dc-b1f3-48cb-a3a6-a093c6f11e2a",
   "metadata": {},
   "source": [
    "## Ingesting and linking data objects to folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0103f0-1c5d-4c36-b9d0-2bd091d12800",
   "metadata": {},
   "source": [
    "Let's first get the URIs to the remotely-stored data objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795d0e1e-fd49-4f47-91e3-dbac119b8680",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "s3 = boto3.resource(\"s3\")\n",
    "bucket = s3.Bucket(\"bernardo-test-bucket-1\")\n",
    "dobject_uris = [\n",
    "    f\"s3://{bucket.name}/{object.key}\"\n",
    "    for object in bucket.objects.filter(Prefix=\"mnist_100/\")\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f806c9a1-d6f8-4cb8-9d69-77e4955421ef",
   "metadata": {},
   "source": [
    "Let's now ingest each of the data objects based on their URI and link them to the relevant metadata.\n",
    "\n",
    "In our case, the metadata we want to link each `DObject` to is a `DFolder` entity so that we can later query data objects based on folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847af7b5-ac6e-4e46-a3dd-405a89e97436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create folder for linking to mnist data objects\n",
    "mnist_folder = ln.DFolder(name=\"mnist\")\n",
    "\n",
    "# create dobjects and link them to mnist folder\n",
    "dobjects = [ln.DObject(data=cloudpath) for cloudpath in dobject_uris]\n",
    "mnist_folder.dobjects = dobjects\n",
    "\n",
    "# ingest all data objects\n",
    "ln.add(mnist_folder)\n",
    "ln.add(dobjects);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "6f775e615af92107a24a8dc5e93fe95ce25fd35096484e8bc738bef7b5ea6eb7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
