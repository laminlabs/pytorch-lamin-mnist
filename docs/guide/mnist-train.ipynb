{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Integrating LaminDB with PyTorch and training an autoencoder"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As part of our demonstration of a simple ML workflow with LaminDB and PyTorch, we now proceed to query the ingested data, prepare a PyTorch dataset, and train our simple autoencoder.\n",
    "\n",
    "This notebook focuses on the key interface between LaminDB and MLOps tools: the dataset abstraction.\n",
    "\n",
    "LaminDB provides fundamental building blocks (metadata-enriched object storage, expressive querying and streaming, and built-in data lineage) that simplify and enhance the process of building modular, special-purpose datasets for any ML workflow.\n",
    "\n",
    "```{note}\n",
    "- For an introduction to this four-part demonstration, please see [LaminDB use case: integrating with PyTorch to train a model on the MNIST dataset](./mnist-intro.ipynb).\n",
    "- For ingesting the MNIST dataset stored locally, please see [Ingesting a remote ML dataset](./mnist-ingest-local.ipynb).\n",
    "- For ingesting the MNIST dataset stored in the cloud, please see [Ingesting a remote ML dataset](./mnist-ingest-remote.ipynb).\n",
    "- For extending the LaminDB schema, please see [Extending the LaminDB schema](./mnist-extend-schema.ipynb).\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the instance we created when ingesting data objects from a local folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lndb\n",
    "\n",
    "lndb.load(\"mnist-remote\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lamindb as ln\n",
    "\n",
    "ln.nb.header()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a custom PyTorch dataset with LaminDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most MLOps tools provide a modular, customizable abstraction over datasets. \n",
    "\n",
    "This is exactly where LaminDB comes into play: it not only simplifies the process of building these abstractions, but also integrates crucial features of decentralized data management (shared metadata layer, built-in data lineage, universal biological/biomedical ontologies) into the ML lifecyle.\n",
    "\n",
    "In the case of PyTorch, [that abstraction is the `torch.utils.data.Dataset` class](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html#creating-a-custom-dataset-for-your-files).\n",
    "\n",
    "In order to build a custom PyTorch `Dataset` class, one must inherit from `torch.utils.data.Dataset` and implement three key methods:\n",
    "- `__init__`: initialize and load relevant data objects.\n",
    "- `__len__`: return the size of the dataset.\n",
    "- `__getitem__`: return a sample (feature-label pair) for model training or testing.\n",
    "\n",
    "Let's see how we can build a custom PyTorch `Dataset` by leveraging the LaminDB API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import ToTensor\n",
    "from PIL import Image\n",
    "\n",
    "# To maximize performance, let's disable tracking of each data object as inputs to this notebook run\n",
    "ln.settings.track_run_inputs_upon_load = False\n",
    "\n",
    "\n",
    "class LNDataset(Dataset):\n",
    "    def __init__(self, dfolder: ln.DFolder):\n",
    "        # query dobjects in the data folder\n",
    "        self.dobjects = (\n",
    "            ln.select(ln.DObject)\n",
    "            .join(ln.DObject.dfolders)\n",
    "            .where(ln.DFolder.id == dfolder.id)\n",
    "        ).all()\n",
    "\n",
    "        # define features and labels\n",
    "        self.feature_dobjects = []\n",
    "        for dobject in self.dobjects:\n",
    "            if dobject.name == \"labels\":  # load and define dataframe with labels\n",
    "                self.img_labels = dobject.load()\n",
    "            else:\n",
    "                self.feature_dobjects += [dobject]\n",
    "\n",
    "        # set key torch.utils.data.Dataset attributes\n",
    "        self.transform = ToTensor()\n",
    "        self.target_transform = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # get feature dobject\n",
    "        dobject = self.feature_dobjects[idx]\n",
    "        # get image from dobject\n",
    "        path = dobject.load()\n",
    "        image = Image.open(path)\n",
    "        # get label from dobject\n",
    "        filename = dobject.name + dobject.suffix\n",
    "        label = self.img_labels.loc[\n",
    "            self.img_labels[\"filename\"] == filename, \"label\"\n",
    "        ].item()\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use our custom dataset class to instantiate a dataset and pass it to train/test dataloaders for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split, DataLoader\n",
    "\n",
    "# create our custom Dataset\n",
    "mnist_folder = ln.select(ln.DFolder, name=\"mnist\").one()\n",
    "mnist_dataset = LNDataset(mnist_folder)\n",
    "\n",
    "# define train and test splits\n",
    "train_subset, test_subset = random_split(mnist_dataset, [80, 20])\n",
    "\n",
    "# create train an test Dataloaders based on splits\n",
    "train_loader = DataLoader(train_subset.dataset)\n",
    "test_loader = DataLoader(test_subset.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Train the model (PyTorch Lightning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now finally train a simple model, just like we would train any other model with PyTorch.\n",
    "\n",
    "Here we train a simple autoencoder for illustration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim, nn\n",
    "from torchmetrics import Accuracy\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "encoder = nn.Sequential(nn.Linear(28 * 28, 64), nn.ReLU(), nn.Linear(64, 3))\n",
    "decoder = nn.Sequential(nn.Linear(3, 64), nn.ReLU(), nn.Linear(64, 28 * 28))\n",
    "\n",
    "\n",
    "class LitAutoEncoder(pl.LightningModule):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.test_accuracy = Accuracy(task=\"multiclass\", num_classes=9)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        x = x.view(x.size(0), -1)\n",
    "        z = self.encoder(x)\n",
    "        x_hat = self.decoder(z)\n",
    "        loss = nn.functional.mse_loss(x_hat, x)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n",
    "\n",
    "\n",
    "autoencoder = LitAutoEncoder(encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(limit_train_batches=100, max_epochs=5)\n",
    "trainer.fit(model=autoencoder, train_dataloaders=train_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "6f775e615af92107a24a8dc5e93fe95ce25fd35096484e8bc738bef7b5ea6eb7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
